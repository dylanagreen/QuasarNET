#!/usr/bin/env python

import time
import glob
from tensorflow.keras.models import load_model
from scipy.interpolate import interp1d
from numpy import in1d, array
import fitsio
import argparse
import numpy as np

from quasarnet.io import read_data
from quasarnet.models import custom_loss
from quasarnet.utils import absorber_IGM, get_spectrum_id_fields, process_preds, Wave

parser=argparse.ArgumentParser()
parser.add_argument('--model',type=str,required=True,
        help='the model, output of qn_train')

parser.add_argument('--data',type=str,required=True,nargs="*",
        help='the data to be inspected, output of parse_data')

parser.add_argument('--data-training',type=str,required=True,nargs="*",
        help='the data used for training')

parser.add_argument('--lmin', type = float, required=False,
                    default = 3600.)

parser.add_argument('--lmax', type = float, required=False,
                    default = 10000.)

parser.add_argument('--dll', type = float, required=False,
                    default = 1.e-3)

parser.add_argument('--spall',type=str,required=False,default=None,
        help='the spall file (same pipeline version as the --data')

parser.add_argument('--lines',type=str,nargs="*",
        required=False,default=['LYA', 'CIV(1548)',
            'CIII(1909)', 'MgII(2796)' ,'Hbeta', 'Halpha'],
        help='the list of emission lines used to train the model'
        )

parser.add_argument('--lines-bal',type=str,nargs="*",
        required=False,default=['CIV(1548)'],
        help='the list of BAL lines'
        )

parser.add_argument('--c-th',type=float,required=False,default=0.4,
        help='threshold confidence value for a line-detection')

parser.add_argument('--n-th',type=int,required=False,default=1,
        help='the threshold number of confident lines to define a quasar')

parser.add_argument('--out-dir',type=str,required=False,default='./',
        help='the output directory')

parser.add_argument('--out-suffix',type=str,required=True,
        help='the output suffix')

parser.add_argument('--make-DRQ', action='store_true', default=False, required=False,
        help='whether to make the DRQ file or not')

parser.add_argument('--mode', type = str, required=False, default='BOSS',
                    choices=['BOSS','DESI','DESI_SPECTRA','DESI_COADD','DESI_SIM'])

parser.add_argument('--model-type', type = str, required=False, default='boxes',
                    choices=['boxes','noboxes'])

args=parser.parse_args()

if args.make_DRQ and args.spall is None:
    raise ValueError('Need a spAll file to make DRQ.')

tid_field = get_tid_field(args.mode)
spid_fields = get_spectrum_id_fields(args.mode)

## Get the testing data.
tids,X,spid0,spid1,spid2 = read_data(args.data, return_spid=True)

## Get the training data.
tids_train,X_train,spid0_train,spid1_train,spid2_train = read_data(args.data_training, return_spid=True)

## Assess how many spectra from the training data are in the test data.
in_train = in1d(tids, tids_train)
print('INFO: found {} spectra from the training sample in the test sample'.format(in_train.sum()))

## Load the model and predict on the test data.
m = args.model
print('loading model {}'.format(m))
if args.model_type == 'noboxes':
    custom_objects = {}
elif args.model_type == 'boxes':
    custom_objects={'custom_loss':custom_loss}
model = load_model(m, custom_objects=custom_objects)
print('predicting...')
start = time.time()
aux = model.predict(X[:,:,None])
print('prediction done')
print('took {:3.1f}s'.format(start-time.time()))

## Load the wavelength grid and process the predictions.
print('processing preds...')
wave = Wave(llmin=np.log10(args.lmin),llmax=np.log10(args.lmax),dll=args.dll)
c_line,z_line,zbest,c_line_bal,z_line_bal =\
        process_preds(aux, args.lines, args.lines_bal,wave=wave,mode=args.model_type)
isqso = (c_line>args.c_th).sum(axis=0)>=args.n_th
print('processing done')

## Make the qnAll file.
print('making qnAll file...')
hout=fitsio.FITS(args.out_dir+'/qnAll-'+args.out_suffix+'.fits','rw',clobber=True)
cols = [tids]
names = [tid_field['TARGETID']]
cols += [spid0,spid1,spid2]
names += [spid_fields['SPID0'],spid_fields['SPID1'],spid_fields['SPID2']]
cols += [zbest, isqso.astype(int), in_train.astype(int)]
names += ['ZBEST','IS_QSO','IN_TRAIN']
cols.append(c_line.T)
names.append('C_LINES')
cols.append(z_line.T)
names.append('Z_LINES')
header=[{'name':'LINE_{}'.format(il),'value':absorber_IGM[l],'comment':l} for il,l in enumerate(args.lines)]
header=[{'name':'LINE_BAL_{}'.format(il),'value':absorber_IGM[l],'comment':l} for il,l in enumerate(args.lines_bal)]
cols.append(c_line_bal.T)
names.append('C_LINES_BAL')
cols.append(z_line_bal.T)
names.append('Z_LINES_BAL')
hout.write(cols,names=names,header=header)
hout.close()
print('done')

## If desired, make the DRQ file.
if args.make_DRQ:

    ## Open the spall file to get supplementary information.
    print('getting info from spall...')
    spall = fitsio.FITS(args.spall)
    tids_spall = spall[1]['THING_ID']
    ra_spall = spall[1]['RA']
    dec_spall = spall[1]['DEC']
    sprim_spall = spall[1]['SPECPRIMARY']
    tid2dat = {t:(r,d,s) for t,r,d,s in zip(tids_spall,ra_spall,
        dec_spall,sprim_spall)}
    spall.close()
    print('done')

    print('making DRQ file...')
    hout = fitsio.FITS(args.out_dir+'/DRQ_QN-'+args.out_suffix+'.fits',
                'rw',clobber=True)
    ra = [tid2dat[t][0] for t in tids]
    ra = array(ra)
    dec = [tid2dat[t][1] for t in tids]
    dec = array(dec)
    sprim = [tid2dat[t][2] for t in tids]
    sprim = array(sprim)
    isqso = isqso & (sprim==1)

    print('INFO: found {} unique quasars'.format(isqso.sum()))

    cols = [tids[isqso]]
    names = [tid_field['TARGETID']]
    cols += [spid0[isqso],spid1[isqso],spid2[isqso]]
    names += [spid_fields['SPID0'],spid_fields['SPID1'],spid_fields['SPID2']]
    isbal = (c_line_bal>args.c_th).sum(axis=0)>=1
    cols += [zbest[isqso], ra[isqso], dec[isqso], isbal[isqso]]
    names += ['Z','RA','DEC','BI_CIV']
    hout.write(cols, names=names)
    print('done')
